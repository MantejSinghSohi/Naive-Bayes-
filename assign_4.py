# -*- coding: utf-8 -*-
"""Assign_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dc3SnB961U5FKwVbVF2r3uEtxxk2nPcR
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

class NB_CLS():

  def __init__(self, alpha):
    self.alpha= alpha

  def fit(self, X, y):
    self.y = y
    self.x= X
    self.n_samples, self.n_features = X.shape
    self.classes = np.unique(y)
    self.n_classes = self.classes.shape[0]
    self.priors = self.prior_prob()

  def prior_prob(self):
    # Calculates prior probabilities for each class in y
    P = np.zeros((self.n_classes))
    _, self.occ = np.unique(self.y,return_counts=True)
    P = self.occ / self.n_samples
    return P

  def prob_calc(self, x, h):
    # x -> input feature vector
    # h -> class
    # The function will calculate probability or likelihood that this input feature belongs to class h
    # (prior prob of class h) * (P(x1|h)) * (P(x2|h)) * (P(x3|h)) ........... (P(XN|h))
    m= x.shape[0]
    y= self.y.flatten()
    label= self.classes[h]
    X_c= self.x[y==label] #select the rows belonging to a particular class-label
    total = X_c.shape[0]
    multi= []

    for i in range(m):
      k= x[i]
      column = X_c[:, i]
      count= np.sum(column==k)
      prob= (count+self.alpha)/(total+self.alpha)
      multi.append(prob)

    value= np.prod(multi)*self.priors[h]

    return value

  def predict(self, X):
    samples, features = X.shape
    self.predict_prob = np.zeros((samples,self.n_classes)) # matrix containing likelihoods of any particular sample being belonging to each class

    for i in range(samples):
      probs = np.zeros((self.n_classes))

      for h in range(self.n_classes):
          probs[h]= self.prob_calc(X[i],h)

      self.predict_prob[i] = probs

    indices = np.argmax(self.predict_prob,axis=1) # calculating maximum value along each row (i.e. class having max prob), for each example , which is the predicted class
    return self.classes[indices]

  def accuracy(self, y_test, predictions):
    samples= y_test.shape[0]
    count =0
    for i in range(samples):
      if (y_test[i]== predictions[i]):
        count = count +1
    accuracy = count/ samples
    return accuracy

# Splitter function divide each attribute value into K equally wide bins spanning the lower and higher values
def splitter(x, k):
    m, n = np.shape(x)
    for i in range(n):
        max_val = np.max(x[:, i])
        min_val = np.min(x[:, i])
        width = (max_val - min_val) / k
        for j in range(m):
            value = x[j, i]
            index = int((value - min_val) / width)
            index = max(min(index, k), 0)
            x[j, i] = index
    return x

_data= pd.read_csv("Iris.csv") # reading from csv
data = _data.sample(frac=1, random_state=809).reset_index(drop=True) # shuffling the data

# dividing into features and targets
features= data.iloc[:, 1:5].values
targets= data.iloc[:, 5].values.reshape(-1,1)

# dividing into k equal bins
_features= splitter(features, 2)
X_train, X_test, y_train, y_test = train_test_split(_features, targets, test_size=0.2, random_state=405)

# Running model for k= 2
accuracy= []
model = NB_CLS(2)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
accuracy_2= model.accuracy(y_test, predictions)
accuracy.append(accuracy_2)

_data= pd.read_csv("Iris.csv") # reading from csv
data = _data.sample(frac=1, random_state=809).reset_index(drop=True) # shuffling the data

# dividing into features and targets
features= data.iloc[:, 1:5].values
targets= data.iloc[:, 5].values.reshape(-1,1)

# dividing into k equal bins
_features= splitter(features, 3)
X_train, X_test, y_train, y_test = train_test_split(_features, targets, test_size=0.2, random_state=405)

# Running model for k= 3
model = NB_CLS(2)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
accuracy_3= model.accuracy(y_test, predictions)
accuracy.append(accuracy_3)

_data= pd.read_csv("Iris.csv") # reading from csv
data = _data.sample(frac=1, random_state=809).reset_index(drop=True) # shuffling the data

# dividing into features and targets
features= data.iloc[:, 1:5].values
targets= data.iloc[:, 5].values.reshape(-1,1)

# dividing into k equal bins
_features= splitter(features, 6)
X_train, X_test, y_train, y_test = train_test_split(_features, targets, test_size=0.2, random_state=405)

# Running model for k= 2
model = NB_CLS(2)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
accuracy_5= model.accuracy(y_test, predictions)
accuracy.append(accuracy_5)

# Experiment 1
# Report the effect of varying the number of bins K in [NB_CLS] on Test data.
K = [2,3,5]
_accuracy = [x * 100 for x in accuracy]

# Making a plot
plt.plot(K, _accuracy, color='blue', linestyle='-', marker='o')

# Add labels and title
plt.xlabel('Hyperparameter K')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Hyperparameter k')
plt.show()

# Find the index of the maximum accuracy value
best_k = accuracy.index(max(accuracy))
Best_k = K[best_k]

print("\nFor K= ", K, "Accuracy values= ", accuracy )
print("Best parameter k based on maximum accuracy:", Best_k)

#Experiment 2
_data= pd.read_csv("Iris.csv") # reading from csv
data = _data.sample(frac=1, random_state=809).reset_index(drop=True) # shuffling the data

# dividing into features and targets
features= data.iloc[:, 1:5].values
targets= data.iloc[:, 5].values.reshape(-1,1)
X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=405)

# adding noise to 10% of the training data
np.random.seed(1200)
size= (int(X_train.shape[0]*0.1), X_train.shape[1])
noise_sample = np.random.normal(loc=0, scale=2, size=size)
random_indices= np.random.randint(0,120,size=12)
X_train[random_indices]= noise_sample

# dividing into k=5 equal bins
X_train = splitter(X_train, 5)
X_test = splitter(X_test, 5)

# Running model for k= 5
accuracy= []
model = NB_CLS(2)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
_accuracy= model.accuracy(y_test, predictions)
accuracy.append(_accuracy)

_data= pd.read_csv("Iris.csv") # reading from csv
data = _data.sample(frac=1, random_state=809).reset_index(drop=True) # shuffling the data

# dividing into features and targets
features= data.iloc[:, 1:5].values
targets= data.iloc[:, 5].values.reshape(-1,1)
X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=405)

# adding noise to 40% of the training data
np.random.seed(1200)
size= (int(X_train.shape[0]*0.4), X_train.shape[1])
noise_sample = np.random.normal(loc=0, scale=2, size=size)
random_indices= np.random.randint(0,120,48)
X_train[random_indices]= noise_sample

# dividing into k=5 equal bins
X_train = splitter(X_train, 5)
X_test = splitter(X_test, 5)

# Running model for k= 5
model = NB_CLS(2)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
_accuracy= model.accuracy(y_test, predictions)
accuracy.append(_accuracy)

_data= pd.read_csv("Iris.csv") # reading from csv
data = _data.sample(frac=1, random_state=809).reset_index(drop=True) # shuffling the data

# dividing into features and targets
features= data.iloc[:, 1:5].values
targets= data.iloc[:, 5].values.reshape(-1,1)
X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=405)

# adding noise to 80% of the training data
np.random.seed(1200)
size= (int(X_train.shape[0]*0.8), X_train.shape[1])
noise_sample = np.random.normal(loc=0, scale=2, size=size)
random_indices= np.random.randint(0,120,size=96)
X_train[random_indices]= noise_sample

# dividing into k=5 equal bins
X_train = splitter(X_train, 5)
X_test = splitter(X_test, 5)

# Running model for k= 5
model = NB_CLS(2)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
_accuracy= model.accuracy(y_test, predictions)
accuracy.append(_accuracy)

_data= pd.read_csv("Iris.csv") # reading from csv
data = _data.sample(frac=1, random_state=809).reset_index(drop=True) # shuffling the data

# dividing into features and targets
features= data.iloc[:, 1:5].values
targets= data.iloc[:, 5].values.reshape(-1,1)
X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=405)

# adding noise to 90% of the training data
np.random.seed(1200)
size= (int(X_train.shape[0]*0.9), X_train.shape[1])
noise_sample = np.random.normal(loc=0, scale=2, size=size)
random_indices= np.random.randint(0,120,size=108)
X_train[random_indices]= noise_sample

# dividing into k=5 equal bins
X_train = splitter(X_train, 5)
X_test = splitter(X_test, 5)

# Running model for k= 5
model = NB_CLS(2)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
_accuracy= model.accuracy(y_test, predictions)
accuracy.append(_accuracy)
__accuracy = [x * 100 for x in accuracy]
noise_per= ["10%","40%","80%","90%"]
print("For Noise percentages= ", K, "Accuracy values= ", __accuracy)
print('''\nTherefore, based on our analysis, we conclude that Naive Bayes exhibits a naivety in its approach and suffers
from a significant bias problem. It is evident that its performance is subpar, with notably low accuracy rates
even under a 10% noise scenario. Surprisingly, under a noise level as high as 80%, it achieves a relatively higher
accuracy of up to 40%, which defies conventional expectations. Consequently, Naive Bayes demonstrates its inherent
naivety in handling noise and bias.
''')